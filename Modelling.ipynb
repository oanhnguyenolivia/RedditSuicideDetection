{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a0d1452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/oanh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/oanh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os, re\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce7d6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Post</th>\n",
       "      <th>Suicidal_label</th>\n",
       "      <th>Sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Am I weird I don t get affected by compliments...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Finally   is almost over  So I can never hear ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I m so lostHello  my name is Adam   and I ve b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226948</th>\n",
       "      <td>227680</td>\n",
       "      <td>I sound like a dudebro but I can t handle my f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226949</th>\n",
       "      <td>227681</td>\n",
       "      <td>Fuck my sister She is such I fucking bitch and...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226950</th>\n",
       "      <td>227682</td>\n",
       "      <td>I ve been suicidal for years and no one knowsT...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226951</th>\n",
       "      <td>227683</td>\n",
       "      <td>My boyfriend is sick so I took some Polaroids ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226952</th>\n",
       "      <td>227684</td>\n",
       "      <td>What would happen to my dog  M What would happ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226953 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               Post  \\\n",
       "0                0  Ex Wife Threatening SuicideRecently I left my ...   \n",
       "1                1  Am I weird I don t get affected by compliments...   \n",
       "2                2  Finally   is almost over  So I can never hear ...   \n",
       "3                3          i need helpjust help me im crying so hard   \n",
       "4                4  I m so lostHello  my name is Adam   and I ve b...   \n",
       "...            ...                                                ...   \n",
       "226948      227680  I sound like a dudebro but I can t handle my f...   \n",
       "226949      227681  Fuck my sister She is such I fucking bitch and...   \n",
       "226950      227682  I ve been suicidal for years and no one knowsT...   \n",
       "226951      227683  My boyfriend is sick so I took some Polaroids ...   \n",
       "226952      227684  What would happen to my dog  M What would happ...   \n",
       "\n",
       "        Suicidal_label  Sentiment_label  \n",
       "0                    0                0  \n",
       "1                    1                1  \n",
       "2                    1                0  \n",
       "3                    0                0  \n",
       "4                    0                0  \n",
       "...                ...              ...  \n",
       "226948               0                0  \n",
       "226949               1                0  \n",
       "226950               0                1  \n",
       "226951               1                0  \n",
       "226952               0                0  \n",
       "\n",
       "[226953 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Dataset_Suicide.csv', lineterminator = '\\n')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5480fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Post</th>\n",
       "      <th>Suicidal_label</th>\n",
       "      <th>Sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Am I weird I don t get affected by compliments...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Finally   is almost over  So I can never hear ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I m so lostHello  my name is Adam   and I ve b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Post  \\\n",
       "0           0  Ex Wife Threatening SuicideRecently I left my ...   \n",
       "1           1  Am I weird I don t get affected by compliments...   \n",
       "2           2  Finally   is almost over  So I can never hear ...   \n",
       "3           3          i need helpjust help me im crying so hard   \n",
       "4           4  I m so lostHello  my name is Adam   and I ve b...   \n",
       "\n",
       "   Suicidal_label  Sentiment_label  \n",
       "0               1                1  \n",
       "1               0                2  \n",
       "2               0                1  \n",
       "3               1                1  \n",
       "4               1                1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with the 'label' column\n",
    "data['Suicidal_label'] = data['Suicidal_label'].map({1: 0, 0: 1})\n",
    "data['Sentiment_label'] = data['Sentiment_label'].map({1: 2, 0: 1, 2: 0})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89051b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226953, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    113534\n",
       "0    113419\n",
       "Name: Suicidal_label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data['Suicidal_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006a3733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34592572\n"
     ]
    }
   ],
   "source": [
    "data['Post'].fillna('default_text', inplace=True)\n",
    "def preprocess(string):\n",
    "    phrase = str(string)\n",
    "    phrase = re.sub('[^a-z]+', ' ', phrase, flags = re.IGNORECASE)\n",
    "    phrase = re.sub('(\\s+)', ' ', phrase)\n",
    "    phrase = re.sub('http\\S+', ' ', phrase)\n",
    "    phrase = phrase.lower()\n",
    "    words_li = ['filler']\n",
    "    li = list(stopwords.words()) + words_li\n",
    "    text_tokens = word_tokenize(phrase)\n",
    "    return \" \".join([word for word in text_tokens if word not in li])\n",
    "print(data['Post'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b297f1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9211270\n"
     ]
    }
   ],
   "source": [
    "data['Post'] = data['Post'].map(lambda string: preprocess(string))\n",
    "print(data['Post'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c51e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136171,) (45391,) (45391,)\n"
     ]
    }
   ],
   "source": [
    "X = data['Post'].values\n",
    "y = data['Suicidal_label'].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size = 0.8, stratify = y)\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, train_size = 0.75, stratify = train_y)\n",
    "print(train_X.shape, test_y.shape, val_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "178297f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Load FastText model (ensure you have FastText installed and the model is downloaded)\n",
    "wv = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0e3dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.key_to_index:\n",
    "            mean.append(wv.get_vector(word))\n",
    "            all_words.add(wv.key_to_index[word])\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a46005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(str(text), language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            word = word.lower()\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d14ef668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "train_X_tokenized = [w2v_tokenize_text(text) for text in train_X]\n",
    "X_train_word_average = word_averaging_list(wv, train_X_tokenized)\n",
    "# Process validation data\n",
    "val_X_tokenized = [w2v_tokenize_text(text) for text in val_X]\n",
    "X_val_word_average = word_averaging_list(wv, val_X_tokenized)\n",
    "# Process testing data\n",
    "test_X_tokenized = [w2v_tokenize_text(text) for text in test_X]\n",
    "X_test_word_average = word_averaging_list(wv, test_X_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11ab9504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.818911 using {'n_neighbors': 10, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Create the KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define the parameters for GridSearch CV\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 10],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Perform GridSearch CV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=knn_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_result = grid_search.fit(X_train_word_average, train_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4a94e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 81.89%\n",
      "Cross-Validation Recall: 81.89%\n",
      "Train accuracy score: 0.8447687099308957\n",
      "Test accuracy score: 0.8234231455574894\n",
      "Train recall score: 0.8447687099308957\n",
      "Test recall score: 0.8234231455574894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.69      0.80     22684\n",
      "           1       0.75      0.96      0.84     22707\n",
      "\n",
      "    accuracy                           0.82     45391\n",
      "   macro avg       0.85      0.82      0.82     45391\n",
      "weighted avg       0.85      0.82      0.82     45391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best KNN model and best parameters\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "best_knn_params = grid_search.best_params_\n",
    "\n",
    "# Perform cross-validation using the best model\n",
    "cv_scores_accuracy = cross_val_score(best_knn_model, X_train_word_average, train_y, cv=5, scoring='accuracy')\n",
    "cv_scores_recall = cross_val_score(best_knn_model, X_train_word_average, train_y, cv=5, scoring='recall_micro')\n",
    "\n",
    "# Print average cross-validation scores for accuracy and recall\n",
    "print(f'Cross-Validation Accuracy: {100 * np.mean(cv_scores_accuracy):.2f}%')\n",
    "print(f'Cross-Validation Recall: {100 * np.mean(cv_scores_recall):.2f}%')\n",
    "\n",
    "# Fit the best model on the training data and make predictions\n",
    "best_knn_model.fit(X_train_word_average, train_y)\n",
    "y_train_pred = best_knn_model.predict(X_train_word_average)\n",
    "y_test_pred = best_knn_model.predict(X_test_word_average)\n",
    "\n",
    "# Calculate and print the train and test accuracy and recall scores\n",
    "print('Train accuracy score:', accuracy_score(train_y, y_train_pred))\n",
    "print('Test accuracy score:', accuracy_score(test_y, y_test_pred))\n",
    "print('Train recall score:', recall_score(train_y, y_train_pred, average='micro'))\n",
    "print('Test recall score:', recall_score(test_y, y_test_pred, average='micro'))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(test_y, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e59df054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/oanh/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.896902 using {'C': 10, 'solver': 'sag'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "logreg = LogisticRegression(n_jobs=1)\n",
    "\n",
    "# Define the parameters for GridSearch CV\n",
    "logreg_params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Perform GridSearch CV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=logreg_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_result = grid_search.fit(X_train_word_average, train_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6478013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 89.69%\n",
      "Cross-Validation Recall: 89.69%\n",
      "Train accuracy score: 0.8979371525508368\n",
      "Test accuracy score: 0.8982838007534534\n",
      "Train recall score: 0.8979371525508368\n",
      "Test recall score: 0.8982838007534534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90     22684\n",
      "           1       0.89      0.91      0.90     22707\n",
      "\n",
      "    accuracy                           0.90     45391\n",
      "   macro avg       0.90      0.90      0.90     45391\n",
      "weighted avg       0.90      0.90      0.90     45391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best Logistic Regression model and best parameters\n",
    "best_logreg_model = grid_search.best_estimator_\n",
    "best_logreg_params = grid_search.best_params_\n",
    "\n",
    "# Perform cross-validation using the best model\n",
    "cv_scores_accuracy = cross_val_score(best_logreg_model, X_train_word_average, train_y, cv=5, scoring='accuracy')\n",
    "cv_scores_recall = cross_val_score(best_logreg_model, X_train_word_average, train_y, cv=5, scoring='recall_micro')\n",
    "\n",
    "# Print average cross-validation scores for accuracy and recall\n",
    "print(f'Cross-Validation Accuracy: {100 * np.mean(cv_scores_accuracy):.2f}%')\n",
    "print(f'Cross-Validation Recall: {100 * np.mean(cv_scores_recall):.2f}%')\n",
    "\n",
    "# Fit the best model on the training data and make predictions\n",
    "best_logreg_model.fit(X_train_word_average, train_y)\n",
    "y_train_pred = best_logreg_model.predict(X_train_word_average)\n",
    "y_test_pred = best_logreg_model.predict(X_test_word_average)\n",
    "\n",
    "# Calculate and print the train and test accuracy and recall scores\n",
    "print('Train accuracy score:', accuracy_score(train_y, y_train_pred))\n",
    "print('Test accuracy score:', accuracy_score(test_y, y_test_pred))\n",
    "print('Train recall score:', recall_score(train_y, y_train_pred, average='micro'))\n",
    "print('Test recall score:', recall_score(test_y, y_test_pred, average='micro'))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(test_y, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc3afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "077b0f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.897320 using {'alpha': 1e-05, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Create the SGD Classifier model\n",
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "# Define the parameters for GridSearch CV\n",
    "sgd_params = {\n",
    "    'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "# Perform GridSearch CV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=sgd_clf, param_grid=sgd_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_result = grid_search.fit(X_train_word_average, train_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8976d6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 89.69%\n",
      "Cross-Validation Recall: 89.33%\n",
      "Train accuracy score: 0.8863340946310154\n",
      "Test accuracy score: 0.8864752924588575\n",
      "Train recall score: 0.8863340946310154\n",
      "Test recall score: 0.8864752924588575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88     22684\n",
      "           1       0.85      0.94      0.89     22707\n",
      "\n",
      "    accuracy                           0.89     45391\n",
      "   macro avg       0.89      0.89      0.89     45391\n",
      "weighted avg       0.89      0.89      0.89     45391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best SGD Classifier model and best parameters\n",
    "best_sgd_model = grid_search.best_estimator_\n",
    "best_sgd_params = grid_search.best_params_\n",
    "\n",
    "# Perform cross-validation using the best model\n",
    "cv_scores_accuracy = cross_val_score(best_sgd_model, X_train_word_average, train_y, cv=5, scoring='accuracy')\n",
    "cv_scores_recall = cross_val_score(best_sgd_model, X_train_word_average, train_y, cv=5, scoring='recall_micro')\n",
    "\n",
    "# Print average cross-validation scores for accuracy and recall\n",
    "print(f'Cross-Validation Accuracy: {100 * np.mean(cv_scores_accuracy):.2f}%')\n",
    "print(f'Cross-Validation Recall: {100 * np.mean(cv_scores_recall):.2f}%')\n",
    "\n",
    "# Fit the best model on the training data and make predictions\n",
    "best_sgd_model.fit(X_train_word_average, train_y)\n",
    "y_train_pred = best_sgd_model.predict(X_train_word_average)\n",
    "y_test_pred = best_sgd_model.predict(X_test_word_average)\n",
    "\n",
    "# Calculate and print the train and test accuracy and recall scores\n",
    "print('Train accuracy score:', accuracy_score(train_y, y_train_pred))\n",
    "print('Test accuracy score:', accuracy_score(test_y, y_test_pred))\n",
    "print('Train recall score:', recall_score(train_y, y_train_pred, average='micro'))\n",
    "print('Test recall score:', recall_score(test_y, y_test_pred, average='micro'))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(test_y, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d13fbb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4256/4256 [==============================] - 2s 440us/step - loss: 0.3227 - accuracy: 0.8724 - val_loss: 0.2706 - val_accuracy: 0.8942\n",
      "Epoch 2/100\n",
      "4256/4256 [==============================] - 2s 431us/step - loss: 0.2758 - accuracy: 0.8931 - val_loss: 0.2519 - val_accuracy: 0.9011\n",
      "Epoch 3/100\n",
      "4256/4256 [==============================] - 2s 421us/step - loss: 0.2629 - accuracy: 0.8986 - val_loss: 0.2479 - val_accuracy: 0.9054\n",
      "Epoch 4/100\n",
      "4256/4256 [==============================] - 2s 424us/step - loss: 0.2559 - accuracy: 0.9016 - val_loss: 0.2428 - val_accuracy: 0.9048\n",
      "Epoch 5/100\n",
      "4256/4256 [==============================] - 2s 426us/step - loss: 0.2507 - accuracy: 0.9037 - val_loss: 0.2366 - val_accuracy: 0.9072\n",
      "Epoch 6/100\n",
      "4256/4256 [==============================] - 2s 423us/step - loss: 0.2479 - accuracy: 0.9043 - val_loss: 0.2341 - val_accuracy: 0.9085\n",
      "Epoch 7/100\n",
      "4256/4256 [==============================] - 2s 422us/step - loss: 0.2447 - accuracy: 0.9058 - val_loss: 0.2328 - val_accuracy: 0.9099\n",
      "Epoch 8/100\n",
      "4256/4256 [==============================] - 2s 420us/step - loss: 0.2424 - accuracy: 0.9069 - val_loss: 0.2303 - val_accuracy: 0.9119\n",
      "Epoch 9/100\n",
      "4256/4256 [==============================] - 2s 430us/step - loss: 0.2406 - accuracy: 0.9074 - val_loss: 0.2298 - val_accuracy: 0.9124\n",
      "Epoch 10/100\n",
      "4256/4256 [==============================] - 2s 427us/step - loss: 0.2383 - accuracy: 0.9082 - val_loss: 0.2256 - val_accuracy: 0.9134\n",
      "Epoch 11/100\n",
      "4256/4256 [==============================] - 2s 430us/step - loss: 0.2370 - accuracy: 0.9090 - val_loss: 0.2325 - val_accuracy: 0.9117\n",
      "Epoch 12/100\n",
      "4256/4256 [==============================] - 2s 423us/step - loss: 0.2360 - accuracy: 0.9094 - val_loss: 0.2247 - val_accuracy: 0.9135\n",
      "Epoch 13/100\n",
      "4256/4256 [==============================] - 2s 421us/step - loss: 0.2344 - accuracy: 0.9097 - val_loss: 0.2233 - val_accuracy: 0.9143\n",
      "Epoch 14/100\n",
      "4256/4256 [==============================] - 2s 427us/step - loss: 0.2330 - accuracy: 0.9105 - val_loss: 0.2238 - val_accuracy: 0.9149\n",
      "Epoch 15/100\n",
      "4256/4256 [==============================] - 2s 425us/step - loss: 0.2310 - accuracy: 0.9115 - val_loss: 0.2225 - val_accuracy: 0.9154\n",
      "Epoch 16/100\n",
      "4256/4256 [==============================] - 2s 421us/step - loss: 0.2309 - accuracy: 0.9111 - val_loss: 0.2214 - val_accuracy: 0.9150\n",
      "Epoch 17/100\n",
      "4256/4256 [==============================] - 2s 418us/step - loss: 0.2300 - accuracy: 0.9120 - val_loss: 0.2211 - val_accuracy: 0.9143\n",
      "Epoch 18/100\n",
      "4256/4256 [==============================] - 2s 419us/step - loss: 0.2280 - accuracy: 0.9119 - val_loss: 0.2203 - val_accuracy: 0.9149\n",
      "Epoch 19/100\n",
      "4256/4256 [==============================] - 2s 434us/step - loss: 0.2281 - accuracy: 0.9130 - val_loss: 0.2196 - val_accuracy: 0.9154\n",
      "Epoch 20/100\n",
      "4256/4256 [==============================] - 2s 430us/step - loss: 0.2270 - accuracy: 0.9132 - val_loss: 0.2186 - val_accuracy: 0.9154\n",
      "Epoch 21/100\n",
      "4256/4256 [==============================] - 2s 420us/step - loss: 0.2264 - accuracy: 0.9131 - val_loss: 0.2199 - val_accuracy: 0.9160\n",
      "Epoch 22/100\n",
      "4256/4256 [==============================] - 2s 420us/step - loss: 0.2258 - accuracy: 0.9144 - val_loss: 0.2208 - val_accuracy: 0.9158\n",
      "Epoch 23/100\n",
      "4256/4256 [==============================] - 2s 419us/step - loss: 0.2257 - accuracy: 0.9132 - val_loss: 0.2198 - val_accuracy: 0.9149\n",
      "Epoch 24/100\n",
      "4256/4256 [==============================] - 2s 421us/step - loss: 0.2251 - accuracy: 0.9139 - val_loss: 0.2185 - val_accuracy: 0.9163\n",
      "Epoch 25/100\n",
      "4256/4256 [==============================] - 2s 422us/step - loss: 0.2234 - accuracy: 0.9147 - val_loss: 0.2200 - val_accuracy: 0.9159\n",
      "Epoch 26/100\n",
      "4256/4256 [==============================] - 2s 421us/step - loss: 0.2242 - accuracy: 0.9144 - val_loss: 0.2248 - val_accuracy: 0.9139\n",
      "Epoch 27/100\n",
      "4256/4256 [==============================] - 2s 418us/step - loss: 0.2237 - accuracy: 0.9142 - val_loss: 0.2176 - val_accuracy: 0.9169\n",
      "Epoch 28/100\n",
      "4256/4256 [==============================] - 2s 419us/step - loss: 0.2224 - accuracy: 0.9145 - val_loss: 0.2172 - val_accuracy: 0.9165\n",
      "Epoch 29/100\n",
      "4256/4256 [==============================] - 2s 428us/step - loss: 0.2216 - accuracy: 0.9155 - val_loss: 0.2177 - val_accuracy: 0.9167\n",
      "Epoch 30/100\n",
      "4256/4256 [==============================] - 2s 422us/step - loss: 0.2209 - accuracy: 0.9156 - val_loss: 0.2177 - val_accuracy: 0.9162\n",
      "Epoch 31/100\n",
      "4256/4256 [==============================] - 2s 421us/step - loss: 0.2207 - accuracy: 0.9155 - val_loss: 0.2162 - val_accuracy: 0.9171\n",
      "Epoch 32/100\n",
      "4256/4256 [==============================] - 2s 419us/step - loss: 0.2205 - accuracy: 0.9153 - val_loss: 0.2166 - val_accuracy: 0.9164\n",
      "Epoch 33/100\n",
      "4256/4256 [==============================] - 2s 418us/step - loss: 0.2189 - accuracy: 0.9168 - val_loss: 0.2172 - val_accuracy: 0.9163\n",
      "Epoch 34/100\n",
      "4256/4256 [==============================] - 2s 424us/step - loss: 0.2189 - accuracy: 0.9161 - val_loss: 0.2156 - val_accuracy: 0.9165\n",
      "Epoch 35/100\n",
      "4256/4256 [==============================] - 2s 425us/step - loss: 0.2189 - accuracy: 0.9157 - val_loss: 0.2170 - val_accuracy: 0.9169\n",
      "Epoch 36/100\n",
      "4117/4256 [============================>.] - ETA: 0s - loss: 0.2185 - accuracy: 0.9166Restoring model weights from the end of the best epoch: 31.\n",
      "4256/4256 [==============================] - 2s 422us/step - loss: 0.2188 - accuracy: 0.9165 - val_loss: 0.2166 - val_accuracy: 0.9164\n",
      "Epoch 36: early stopping\n",
      "Test loss: 0.21616798639297485\n",
      "Test accuracy: 0.9171421527862549\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Assuming word embeddings size is 300 for FastText model\n",
    "embedding_dimension = 300\n",
    "\n",
    "# Ensure that X_train_word_average has the correct shape\n",
    "X_train_word_average = word_averaging_list(wv, train_X_tokenized)\n",
    "X_train_word_average = np.array(X_train_word_average)\n",
    "X_train_word_average = X_train_word_average.reshape(X_train_word_average.shape[0], embedding_dimension)\n",
    "\n",
    "# Ensure that train_y has the correct shape\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "# Build the Recursive Neural Network (RNN) model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(300,)))  # Assuming word embeddings size is 300 for FastText model\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification, hence 1 neuron with sigmoid activation\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_word_average, train_y, batch_size=32, epochs=100, \n",
    "                    verbose=1, callbacks=[early_stopping], validation_data=(X_test_word_average, test_y))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(X_test_word_average, test_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81c1c58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4256/4256 [==============================] - 1s 182us/step\n",
      "1419/1419 [==============================] - 0s 198us/step\n",
      "Train accuracy score: 0.9251162141718868\n",
      "Test accuracy score: 0.9171421647463154\n",
      "Train recall score: 0.9251162141718868\n",
      "Test recall score: 0.9171421647463154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92     22684\n",
      "           1       0.91      0.92      0.92     22707\n",
      "\n",
      "    accuracy                           0.92     45391\n",
      "   macro avg       0.92      0.92      0.92     45391\n",
      "weighted avg       0.92      0.92      0.92     45391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the training and testing data\n",
    "y_train_pred = model.predict(X_train_word_average)\n",
    "y_train_pred = (y_train_pred > 0.5).astype(int)\n",
    "\n",
    "y_test_pred = model.predict(X_test_word_average)\n",
    "y_test_pred = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "# Print the accuracy and recall scores for both training and testing data\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report\n",
    "\n",
    "print('Train accuracy score:', accuracy_score(train_y, y_train_pred))\n",
    "print('Test accuracy score:', accuracy_score(test_y, y_test_pred))\n",
    "print('Train recall score:', recall_score(train_y, y_train_pred, average='micro'))\n",
    "print('Test recall score:', recall_score(test_y, y_test_pred, average='micro'))\n",
    "\n",
    "# Print the classification report for the testing data\n",
    "print(classification_report(test_y, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37888de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step\n",
      "Text: I am tired of my life i want to end my life\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: I have a nice day\n",
      "Predicted Label: 0\n",
      "\n",
      "Text: I went to the grocery store this morning and they had a fruit that I'd never seen before. I asked an employee what it was and she didn't know so we tried to Google it but nothing came up! At that point I as too curious to give up, so bought it and said I'd figure it out later. They didn't even know how to charge me so it only cost $1! I'm gonna try it later, maybe do a cheese and charcuterie board with some wine and be fancy for no reason lol.\n",
      "Predicted Label: 0\n",
      "\n",
      "Text: I'm autistic and I've had enough\n",
      "Predicted Label: 0\n",
      "\n",
      "Text: Ok I've had enough and I don't want to be autistic anymore\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: I think that this is goodbye\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: Someone Fucking Help Me\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: I don’t know why I’m so sad\n",
      "Predicted Label: 0\n",
      "\n",
      "Text: But I also haven’t been able to shake this feeling for the past couple of months that I just want to fucking explode. I want to go off the deep end and just throw everything I have away. I want to hurt the people I love so that one day they will throw me away too. It’s like everyday I do something good for myself is just me putting off the inevitable crash and burn. I just don’t want to give a fuck anymore and I don’t know why when everything has been going so well.\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: Like, I don’t know. I just don’t know. I’m pretty much on the verge of tears over nothing. There might be something, but I guess my brain just isn’t showing me. I just lay in my room and feel lonely and depressed for hours on end. It gets repetitive sometimes. I’m sick of this feeling.\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: I'm going to explode\n",
      "Predicted Label: 0\n",
      "\n",
      "Text: my husband is terminally i’ll and is constantly in pain for over 2.5 years now I can’t take it anymore\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: I am 54 with complex mental health and physical health issues none of them terminal  (at least in the books of law). So I have no access to VAD. I have no want to be in this world any longer - I cannot live in my own head and nor with the physical pain I am in with back, shoulder, migraine issues ..\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: I'm 58 and I can't do this anymore. Everyday is misery. I live with a woman who uses me fir money and my prescriptions. So I as in horrendous pain most of the time. I'm always at risk of going to jail if i say no to her. She takes every cent i have . I have no where to go but where I am going tonight. If anyone reads this thanks for taking the time. I was here.\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: I love my life\n",
      "Predicted Label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New texts to predict\n",
    "new_texts = ['I am tired of my life i want to end my life', \n",
    "             'I have a nice day',\n",
    "             \"I went to the grocery store this morning and they had a fruit that I'd never seen before. I asked an employee what it was and she didn't know so we tried to Google it but nothing came up! At that point I as too curious to give up, so bought it and said I'd figure it out later. They didn't even know how to charge me so it only cost $1! I'm gonna try it later, maybe do a cheese and charcuterie board with some wine and be fancy for no reason lol.\",\n",
    "             \"I'm autistic and I've had enough\",\n",
    "             \"Ok I've had enough and I don't want to be autistic anymore\",\n",
    "            'I think that this is goodbye',\n",
    "            'Someone Fucking Help Me',\n",
    "            'I don’t know why I’m so sad',\n",
    "             \"But I also haven’t been able to shake this feeling for the past couple of months that I just want to fucking explode. I want to go off the deep end and just throw everything I have away. I want to hurt the people I love so that one day they will throw me away too. It’s like everyday I do something good for myself is just me putting off the inevitable crash and burn. I just don’t want to give a fuck anymore and I don’t know why when everything has been going so well.\",\n",
    "             \"Like, I don’t know. I just don’t know. I’m pretty much on the verge of tears over nothing. There might be something, but I guess my brain just isn’t showing me. I just lay in my room and feel lonely and depressed for hours on end. It gets repetitive sometimes. I’m sick of this feeling.\",\n",
    "            \"I'm going to explode\",\n",
    "            \"my husband is terminally i’ll and is constantly in pain for over 2.5 years now I can’t take it anymore\",\n",
    "            \"I am 54 with complex mental health and physical health issues none of them terminal  (at least in the books of law). So I have no access to VAD. I have no want to be in this world any longer - I cannot live in my own head and nor with the physical pain I am in with back, shoulder, migraine issues ..\",\n",
    "            \"I'm 58 and I can't do this anymore. Everyday is misery. I live with a woman who uses me fir money and my prescriptions. So I as in horrendous pain most of the time. I'm always at risk of going to jail if i say no to her. She takes every cent i have . I have no where to go but where I am going tonight. If anyone reads this thanks for taking the time. I was here.\",\n",
    "            \"I love my life\"]\n",
    "# Preprocess the new texts\n",
    "new_texts_preprocessed = [preprocess(text) for text in new_texts]\n",
    "\n",
    "new_texts_tokenized = [w2v_tokenize_text(text) for text in new_texts_preprocessed]\n",
    "\n",
    "# Generate word embeddings for new texts\n",
    "new_word_average = word_averaging_list(wv, new_texts_tokenized)\n",
    "new_word_average = new_word_average.reshape(-1, wv.vector_size)\n",
    "\n",
    "# Predict labels for new data\n",
    "new_predictions = model.predict(new_word_average)\n",
    "new_predictions = np.round(new_predictions).flatten()\n",
    "\n",
    "# Convert new_predictions to an array of integers\n",
    "new_predictions = new_predictions.astype(int)\n",
    "\n",
    "# Inverse transform the predictions to get original labels\n",
    "new_predictions_labels = le.inverse_transform(new_predictions)\n",
    "\n",
    "# Print the predicted labels for the new texts\n",
    "for text, label in zip(new_texts, new_predictions_labels):\n",
    "    print(f'Text: {text}\\nPredicted Label: {label}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
